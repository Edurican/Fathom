# [가상 메모리]

> 📅 2025.12.16 | 🧑‍💻 이관호 | 🏷️ os, virtual memory

---

## 🎯 개요

### 이 주제를 선택한 이유

RAM은 한정적인데 어떻게 많은 프로세스를 뛰울 수 있는지 궁금하였습니다.

### 목표

이 발표를 통해:

- 가상 메모리란 무엇인지 무엇인가?
- 페이징이란 무엇인가?

---

## 📖 본문

# 1. 의문

RAM이 16GB인데 어떻게 수 많은 프로세스를 뛰울 수 있었을까? </br>
이러한 궁금점이 생겨 가상 메모리에 대해 조사하였습니다.

# 2. 등장한 이유

이전에는 RAM에 바로 프로세스를 뛰우는 방식을 택하였다고 합니다. </br>
그럼 몇가지 문제가 발생하게 됩니다.
1. 물리 메모리 보다 크면 실행 불가 
2. 같은 메모리 쓰면 충돌 
3. 메모리 훔쳐볼 수 있음 
4. 외부 단편화 발생

다른 프로세스에 영향이 가는 방식은 큰 문제였기에 해결방안이 필요했습니다.

# 3. 가상 메모리

각 프로세스가 독립된 메모리 공간을 가지고 있다는 착각 </br>
가상 메모리가 등장하게 되면서 다음의 문제가 해소되었습니다. </br>

가상 메모리로 인해 기존 문제를 해결하였습니다
1. 독립적인 메모리 주소 공간을 제공
2. 다른 프로세스의 메모리에 접근하지 못하게 보호
3. 외부 단편화 해결

# 4. 단편화

### 단편화

메모리 공간이 작은 조각들로 나뉘어져서 전체적으로는 충분한 공간이 있음에도 불구 효율적으로 사용할 수 없는 상태를 의미 </br>

이로 인해 다양한 크기의 빈공간들이 생기게 되고 외부 단편화와 내부 단편화가 발생되게 됨

### 내부 단편화

할당된 메모리 블록 내부에서 실제로 사용되지 않는 공간이 생기는 현상 </br>
메모리를 고정된 크기의 블록으로 나누어 할당할 때 주로 발생 </br>

하나의 할당된 크기가 10인데 5밖에 안쓴다면 5는 낭비가 되는 현상 </br>
주로 고정크기 할당 방식에서 발생 </br>

내부 단편화는 해결은 안되고 최소화 할 수 있다

- 블록 크기를 작게하기
    - 다양한 크기의 페이지를 지원, 단편화를 줄이면서 효율적성 유지가능
    - 블록 크기를 너무 작하게 하면 관리 오버헤드 증가

### 외부 단편화

메모리에 충분한 총 공간이 있음에도 불구하고  이 공간들이 작은 조각으로 흩어져 있어서 큰 프로세스를 할당할 수 없는 현상 </br>

실질적으로 10을 사용할 수 있지만 연속적이지 않아 사용하지 못하는 현상 </br>
주로 가변 크기 할당 방식에서 발생

외부 단편화는 해결이 가능하다
- 압축:
    - 빈공간들을 하나의 큰 연속적인 공간으로 만드는 방법
    - 하지만 프로세스 주소를 다시 계산해야하여 비용이 많이 든다
- 배치 전략 개선
    - 최초 적합, 최적 적합, 최악 적합등의 알고리즘을 사용
    - 빈 공간에 프로세스를 배치할 때 단편화를 최소화하려고 노력
- 페이징
    - 가상 메모리와 물리 메모리를 1:1로 매핑 

# 5. 페이징

컴퓨터가 대용량 데이터를 효율적으로 관리하기 위한 메모리 관리 기법 </br>
핵심 아이디어는 메모리를 고정된 크기의 작은 블록으로 나누어 관리 </br>

가상 메모리 블록은 페이지, 물리 메모리 블록을 프레임이라 부르며 1:1 대응되어 있고 보통 4KB임 </br>
운영체제는 페이지를 프레임에 매핑하여 실제 값을 얻어온다.

이를 가능하게 하는 것은 **페이지 테이블**이다

가상 주소는 두 부분으로 구성됨 </br>

상위 비트는 **페이지 번호(Page Number)**  </br>
페이지 테이블에서 해당 페이지가 어느 프레임에 있는지 찾는 인덱스

하위 비트는 **오프셋(Offset)**  </br>
해당 페이지/프레임 내에서의 위치를 나타냄

이를 이용하여 </br>
CPU의 MMU가 페이지 테이블을 참조 </br>
페이지 번호를 프레임 번호로 변환 </br>
오프셋은 그대로 유지하여 최종 물리 주소를 계산함  </br>

페이징의 장점
- 외부 단편화가 없음
- 메모리 보호가 용이
- 메모리 공유가 효율적

# 6. TLB

현대 프로세스는 초당 수십억 번의 명령을 실행하고 모든 명령어가 메모리에 접근해야하는데 이로인해 많은 주소 변환이 발생 </br>
메모리 접근은 페이지 테이블에 접근, 물리 메모리에 대한 접근 등 성능 오버헤드가 심하다  </br>

해당 성능을 해결할 수 있게 만들어진 특별한 하드웨어 “캐시”이다 </br>
최근에 변환한 주소들을 빠른 메모리에 저장해두자는 것이 핵심이다  </br>
CPU 캐시는 메인 메모리보다 빠르고, TLB는 페이지 테이블 접근보다 빠르다 </br>

동작 원리
1. 프로세스가 가상 주소에 접근
2. MMU는 먼저 TLB를 확인
3. 해당 가상 페이지 번호가 TLB에 있다면 TLB 히트라고 한다
   1. 저장된 물리 프레임 번호를 즉시 사용하여 주소 변환을 완료
   2. 추가적인 메모리 접근 없이 바로 원하는 데이터에 접근 가능
4. 만약 해당 가상 페이지 번호가 없다면 TLB 미스라고 한다
   1. 기존 방식대로 페이지 테이블에 접근해서 주소 변환을 수행
   2. 변환이 완료된 후에 그걸 TLB에 저장

# 7. 요구 페이징

페이지가 실제로 필요할 때 만 물리 메모리로 가져오는 방식 </br>
이를 통해 메모리를 절약하고, 효율적으로 관리할 수 있다.

단계
1. 프로세스가 주소 X에 접근하려고 한다
2. MMU가 페이지 테이블을 확인한다
3. 해당 페이지가 물리 메모리에 있다면 주소 변환을 수행, 없다면 페이지 폴트 처리
4. 물리 메모리에 빈 프레임이 있는지 확인 없다면 페이지 교체 알고리즘 실행
5. 디스크에서 필요한 페이지를 읽어와서 메모리에 올림
6. 페이지 테이블을 업데이트

# 8. 페이지 폴트

프로세스는 동작할 때 모든 메모리를 사용하지 않음 </br>
그래서 페이징을 통해 필요한 메모리는 가져오고 필요없는 메모리는 반납함

만약에 원하는 페이지의 Valid Bit가 0이라면 없다고 판단한다 </br>
운영체제는 해당 페이지를 디스크(SSD, HDD)에서 읽어 빈 프레임에 적재한다.

빈 프레임이 없다면 페이지 교체 알고리즘을 통해 스왑인 아웃한다

# 9. 페이지 교체 알고리즘

어떤 페이지를 내보낼 것인가?

- FIFO: 가장 먼저 메모리에 들어온 페이지를 가장 먼저 내보낸다
- LRU: 가장 최근에 사용되지 않은 페이지를 교체한다
- LFU: 가장 적게 사용된(참조된) 페이지를 교체한다
- 최적: 미래에 가장 늦게 사용될 페이지를 교체하는 이론적 알고리즘

결론 적으로는 LRU의 하이브리드 느낌이다
비트 카운터, 시간을 사용한다
완벽한 알고리즘보다는 실용적이고 효율적인 알고리즘이 중요하다

# 10. 스레싱

시스템이 실제 작업보다 페이지 교체 작업에 더 많은 시간을 소비하는 현상 </br>
페이지 폴트가 지속적으로 발생함에 따라 HDD,SSD에서 읽어오는 작업을 반복함으로 인해 성능 저하

이를 해결하기 위한 방법 중 하나는 **워킹셋**이다

---

## 🎬 결론

### 핵심 요약

가상 메모리는 필요한 것만, 필요할 때만 로드한다.

### 느낀 점

가상 메모리를 통해 다수의 프로세스가 동작되는 원리를 알게되었습니다.

추가적으로 공부할 것:
- 세그멘테이션
- 페이지 테이블 구조
- 워킹 셋

---

## 📚 References
